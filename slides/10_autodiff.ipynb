{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are lots of applications in engineering that requires the computation of the derivative of a function. The simplest example is Newton's method which is a fast iterative algorithm that finds the root of a nonlinear equation:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "$$\n",
    "\n",
    "There are, of course, more complicated applications where (apart from the function values) the derivatives are required, for instance training a neural network requires the minimization of a cost function by gradient descent which requires the derivatives of the cost function with respect to the parameters (weights, biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to calculate the derivative of a function?\n",
    "\n",
    "* symbolic differentiation: either the derivative of $f$ is known exactly in advance or a computer algebraic program calculates it\n",
    "* numerical differentiation: approximate the value of $f'(x)$ with finite differences:\n",
    "$$\n",
    "f'(x)\\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "* automatic differentiation: use a mechanism that calculates the derivative together with the function value without any extra effort, that is, automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dual numbers\n",
    "\n",
    "A dual number can be represented as a symbolic expression $a + b\\cdot\\varepsilon$, where $\\varepsilon^2 = 0$. More precisely, a dual number is a pair of real numbers $(a, b)$, such that addition, subtraction, multiplication and division is defined as follows:\n",
    "\n",
    "* $(a + b\\varepsilon) + (c + d\\varepsilon) = (a + c) + (b + d)\\varepsilon$\n",
    "* $(a + b\\varepsilon) \\cdot (c + d\\varepsilon) = ac + (ad + bc)\\varepsilon$\n",
    "* $-(a + b\\varepsilon) = -a + (-b)\\varepsilon$\n",
    "* $(a + b\\varepsilon) \\,/\\, (c + d\\varepsilon) = \\frac{a}{c} + \\frac{bc - ad}{c^2}\\varepsilon$\n",
    "\n",
    "The dual numbers form a two-dimensional commutative unital associative algebra over the real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us assume that $p$ is a polynomial of degree $n$:\n",
    "\n",
    "$$\n",
    "p(x) = a_0 + a_1x + a_2x^2 + \\ldots + a_nx^n.\n",
    "$$\n",
    "\n",
    "Let $x = a + b\\epsilon$ be a dual number, then\n",
    "\n",
    "$$\n",
    "p(x) = p(a + b\\epsilon) = p(a) + b\\cdot p'(a)\\cdot\\epsilon.\n",
    "$$\n",
    "\n",
    "Then for any analytical function $f$, the dual number $(a, b)$ is mapped to the dual number $(f(a), bf'(a))$. This means that by extending the usual function definitions to dual numbers, the derivative is calculated along the function value with no extra effort.\n",
    "\n",
    "As an example, the trigonometric sine function should be overriden with the following new definition:\n",
    "\n",
    "$$\n",
    "\\sin x = \\sin\\,(a, b) := (\\sin a, b\\cdot cos a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Dual:\n",
    "    def __init__(self, value, derivative):\n",
    "        self.value = value\n",
    "        self.derivative = derivative\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.value} + {self.derivative}*Îµ'\n",
    "        \n",
    "    def __add__(self, that):\n",
    "        return Dual(self.value + that.value, self.derivative + that.derivative)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return Dual(-self.value, -self.derivative)\n",
    "    \n",
    "    def __sub__(self, that):\n",
    "        return self + (-that)\n",
    "    \n",
    "    def __mul__(self, that):\n",
    "        return Dual(self.value*that.value, self.value*that.derivative + self.derivative*that.value)\n",
    "    \n",
    "    def __truediv__(self, that):\n",
    "        a = self.value / that.value\n",
    "        b = (self.derivative*that.value - self.value*that.derivative) / (that.value ** 2)\n",
    "        return Dual(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def sin(d):\n",
    "    # sin x = sin(a, b) := (sin a, b * cos a).\n",
    "    return Dual(math.sin(d.value), math.cos(d.value) * d.derivative)\n",
    "\n",
    "\n",
    "def cos(d):\n",
    "    return Dual(math.cos(d.value), -math.sin(d.value) * d.derivative)\n",
    "\n",
    "\n",
    "def tan(d):\n",
    "    return Dual(math.tan(d.value), d.derivative / math.pow(math.cos(d.value), 2))\n",
    "\n",
    "\n",
    "def cot(d): \n",
    "    return Dual(1, 0) / tan(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Can you implement the following functions for dual numbers?\n",
    "\n",
    "def log(d):\n",
    "    pass\n",
    "\n",
    "\n",
    "def sqrt(d):\n",
    "    pass\n",
    "\n",
    "\n",
    "def exp(d):\n",
    "    pass\n",
    "\n",
    "\n",
    "def power(d, alpha):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "f(x) = \\sqrt{\\log x}, \\qquad f'(3) = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "f'(x) = \\frac{1}{2}\\frac{1}{\\sqrt{\\log x}}\\cdot\\frac{1}{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(d):\n",
    "    return sqrt(log(d))\n",
    "\n",
    "\n",
    "x = Dual(3, 1)\n",
    "y = f(x)\n",
    "\n",
    "print(f'Function value at 3: {y.value}')\n",
    "print(f'Derivative value at 3: {y.derivative}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Links:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Dual_number\n",
    "\n",
    "https://en.wikipedia.org/wiki/Automatic_differentiation\n",
    "\n",
    "From scratch: reverse-mode automatic differentiation (in Python)\n",
    "https://sidsite.com/posts/autodiff/\n",
    "\n",
    "Automatic differentiation in machine learning: a survey\n",
    "https://arxiv.org/abs/1502.05767\n",
    "\n",
    "What is Automatic Differentiation?\n",
    "https://www.youtube.com/watch?time_continue=2&v=wG_nF1awSSY&feature=emb_logo\n",
    "\n",
    "Reverse mode automatic differentiation\n",
    "https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To differentiate automatically: \n",
    "* TensorFlow needs to remember what operations happen in what order during the forward pass. \n",
    "* During the backward pass, it traverses this list of operations in reverse order to compute gradients.\n",
    "\n",
    "TensorFlow provides the ```tf.GradientTape``` API for automatic differentiation.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "\n",
    "https://www.tensorflow.org/guide/autodiff\n",
    "\n",
    "https://www.tensorflow.org/guide/advanced_autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This is the same function we have tested with our manually implemented Dual class\n",
    "def f(x):\n",
    "    return tf.sqrt(tf.math.log(x))\n",
    "\n",
    "\n",
    "x = tf.Variable(3.0, dtype=tf.float64)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dx = tape.gradient(y, x)\n",
    "\n",
    "print(dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/log_reg_1.txt\") as f:\n",
    "    X = []\n",
    "    y = []\n",
    "    for line in f:\n",
    "        x0, x1, label = line.split(',')\n",
    "        X.append((float(x0), float(x1)))\n",
    "        y.append(int(label))\n",
    "        \n",
    "X = np.array(X)\n",
    "y = np.expand_dims(np.array(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Copied from the logistic regression part\n",
    "def activation(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def calc_gradient(X, y, w, b):\n",
    "    m = len(X)\n",
    "    A = activation(np.matmul(X, w) + b)\n",
    "    cost = (-1 / m) * np.sum(np.multiply(y, np.log(A)) + np.multiply(1 - y, np.log(1 - A)))\n",
    "    \n",
    "    dZ = A - y\n",
    "    dw = (1 / m) * np.matmul(X.T, dZ)\n",
    "    db = (1 / m) * np.sum(dZ)\n",
    "    return cost, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using zero initial values calculate the gradient of the loss function with respect to w and b\n",
    "\n",
    "_, nr_features = X.shape\n",
    "w = np.zeros((nr_features, 1), dtype=np.float_)\n",
    "b = 0.0\n",
    "\n",
    "_, dw, db = calc_gradient(X, y, w, b)\n",
    "\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate dw and db without explicit derivative calculation\n",
    "\n",
    "w = tf.Variable(tf.zeros((nr_features, 1), dtype=tf.float64), name='w')\n",
    "b = tf.Variable(0.0, dtype=tf.float64, name='b')\n",
    "x = tf.Variable(X)\n",
    "m, _ = x.shape\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = tf.add(tf.matmul(x, w), b)\n",
    "    a = tf.sigmoid(z)\n",
    "    loss = tf.reduce_mean(tf.losses.binary_crossentropy(y_true=y, y_pred=a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "\n",
    "print(dl_dw.numpy())\n",
    "print(dl_db.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/forward_mode_autodiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/reverse_mode_autodiff.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
